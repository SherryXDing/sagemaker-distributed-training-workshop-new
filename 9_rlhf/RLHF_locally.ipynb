{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a069b0",
   "metadata": {},
   "source": [
    "```\n",
    "Author: Ehsan Kamalinejad (EK)\n",
    "Created: 2023-02-27\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757937bc",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "A training pipeline to finetune a generative model to create IMDb reviews with positive sentiment according to the [OpenAI RLHF paper](https://arxiv.org/abs/1909.08593)(please see section 3).\n",
    "\n",
    "### Requirements\n",
    "- `tqdm`\n",
    "- `omegaconf`\n",
    "- `dataclasses`\n",
    "- `torchtyping`\n",
    "- `datasets`\n",
    "- `transformers`\n",
    "- `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806219f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from omegaconf import DictConfig\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "from typing import Iterable, Sequence, List\n",
    "\n",
    "from torchtyping import TensorType\n",
    "\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63f4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train': {\n",
    "        'seed': 2023,\n",
    "        'seq_length': 1024,\n",
    "        'epochs': 50,\n",
    "        'total_steps': 5000,\n",
    "        'batch_size': 64,\n",
    "        'eval_interval': 100,\n",
    "        'model_device':'cuda:0',\n",
    "        'ref_model_device':'cpu',\n",
    "        'reward_model_device':'cpu'},\n",
    "    'model': {\n",
    "        'model_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'tokenizer_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'num_layers_unfrozen': 1},\n",
    "    'optimizer': {\n",
    "        'name': 'adamw',\n",
    "        'kwargs': {'lr': 0.0001,\n",
    "        'betas': [0.9, 0.95],\n",
    "        'eps': 1e-08,\n",
    "        'weight_decay': 1e-06}},\n",
    "    'scheduler': {\n",
    "        'name': 'cosine_annealing',\n",
    "        'kwargs': {\n",
    "            'T_max': 10000, 'eta_min': 0.0001}},\n",
    "    'method': {\n",
    "        'use_whitening': True,\n",
    "        'prompt_size': 10,\n",
    "        'num_rollouts': 128,\n",
    "        'chunk_size': 128,\n",
    "        'ppo_epochs': 4,\n",
    "        'kl_coef': 0.05,\n",
    "        'horizon': 10000,\n",
    "        'gamma': 1,\n",
    "        'lam': 0.95,\n",
    "        'cliprange': 0.2,\n",
    "        'cliprange_value': 0.2,\n",
    "        'vf_coef': 1,\n",
    "        'scale_reward': False,\n",
    "        'ref_mean': None,\n",
    "        'ref_std': None,\n",
    "        'cliprange_reward': 10,\n",
    "        'gen_kwargs': {\n",
    "            'max_new_tokens': 60,\n",
    "            'top_k': 0,\n",
    "            'top_p': 1.0,\n",
    "            'do_sample': True}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8d10c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = DictConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config.train.seed)\n",
    "np.random.seed(config.train.seed)\n",
    "torch.manual_seed(config.train.seed)\n",
    "torch.cuda.manual_seed(config.train.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2080a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptPipeline():\n",
    "    def __init__(self, prompts: List[str], max_prompt_length: int, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        prompts = tokenizer(prompts).input_ids\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [prompt[-max_prompt_length:] for prompt in prompts]\n",
    "        self.prompts = [{\"input_ids\": prompt, \"attention_mask\": [1] * len(prompt)} for prompt in self.prompts]\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        return self.prompts[ix]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle=False) -> DataLoader:\n",
    "        collate_fn = DataCollatorWithPadding(self.tokenizer)\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e497c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPORLElement:\n",
    "    query_tensor: TensorType[\"query_size\"]\n",
    "    response_tensor: TensorType[\"response_size\"]\n",
    "    logprobs: TensorType[\"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"response_size\"]\n",
    "    rewards: TensorType[\"response_size\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPORLBatch:\n",
    "    query_tensors: TensorType[\"batch_size\", \"query_size\"]\n",
    "    response_tensors: TensorType[\"batch_size\", \"response_size\"]\n",
    "    logprobs: TensorType[\"batch_size\", \"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"batch_size\", \"response_size\"]\n",
    "    rewards: TensorType[\"batch_size\", \"response_size\"]\n",
    "\n",
    "\n",
    "class PPORolloutStorage():\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.history: Iterable[PPORLElement] = [None]\n",
    "\n",
    "    def push(self, exps: Iterable[PPORLElement]):\n",
    "        self.history += exps\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __getitem__(self, index: int) -> PPORLElement:\n",
    "        return self.history[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.history)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "        def collate_fn(elems: Iterable[PPORLElement]):\n",
    "            return PPORLBatch(\n",
    "                pad_sequence(\n",
    "                    [elem.query_tensor.flip(0) for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ).flip(1),\n",
    "                pad_sequence(\n",
    "                    [elem.response_tensor for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.logprobs for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.values for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.rewards for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return DataLoader(self, batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28d570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    var, mean = torch.var_mean(x)\n",
    "    return (x - mean) * torch.rsqrt(var + 1e-8)\n",
    "\n",
    "\n",
    "def gae(\n",
    "    values,\n",
    "    rewards,\n",
    "):\n",
    "    advantages = torch.zeros_like(rewards, device=rewards.device)\n",
    "    last_advantage = 0\n",
    "    last_value = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(rewards.shape[1])):\n",
    "            delta = rewards[:, t] + config.method.gamma * last_value - values[:, t]\n",
    "            last_advantage = delta + config.method.gamma * config.method.lam * last_advantage\n",
    "            advantages[:, t] = last_advantage\n",
    "            last_value = values[:, t]\n",
    "\n",
    "        returns = advantages + values\n",
    "    \n",
    "    if config.method.use_whitening:\n",
    "        advantages = whiten(advantages)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5575874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    logprobs,     \n",
    "    values,       \n",
    "    old_logprobs, \n",
    "    old_values,   \n",
    "    advantages,   \n",
    "    returns,      \n",
    "    mask,         \n",
    "):\n",
    "\n",
    "    values_clipped = torch.clamp(\n",
    "        values,\n",
    "        old_values - config.method.cliprange_value,\n",
    "        old_values + config.method.cliprange_value,\n",
    "    )\n",
    "    \n",
    "    n = mask.sum()\n",
    "    \n",
    "    vf_loss1 = (values - returns) ** 2\n",
    "    vf_loss2 = (values_clipped - returns) ** 2\n",
    "    vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / n\n",
    "\n",
    "    log_ratio = (logprobs - old_logprobs) * mask\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    pg_loss1 = -advantages * ratio\n",
    "    pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - config.method.cliprange, 1.0 + config.method.cliprange)\n",
    "    pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / n\n",
    "    pg_clipfrac = torch.sum((pg_loss2 > pg_loss1).float() * mask) / n\n",
    "\n",
    "    loss = pg_loss + config.method.vf_coef * vf_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811ddf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch):\n",
    "    model_device = next(model.parameters()).device\n",
    "    query_tensors = batch.query_tensors.to(model_device)\n",
    "    response_tensors = batch.response_tensors.to(model_device)\n",
    "    old_logprobs = batch.logprobs.to(model_device)\n",
    "    old_values = batch.values.to(model_device)\n",
    "    old_rewards = batch.rewards.to(model_device)\n",
    "    \n",
    "    response_length = old_rewards.shape[1]\n",
    "\n",
    "    advantages, returns = gae(old_values, old_rewards)\n",
    "\n",
    "    tokens, attention_mask, position_ids = get_model_inputs(query_tensors, response_tensors, tokenizer.pad_token_id)\n",
    "\n",
    "    logits, values_pred = model(tokens,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids)\n",
    "    values_pred = values_pred[:, :-1]\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], tokens[:, 1:])\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    start = query_tensors.shape[1] - 1\n",
    "    end = start + response_length\n",
    "    logprobs, values_pred, mask = (\n",
    "        logprobs[:, start:end],\n",
    "        values_pred[:, start:end],\n",
    "        attention_mask[:, start:end],\n",
    "    )\n",
    "\n",
    "    loss = ppo_loss(\n",
    "        logprobs=logprobs,\n",
    "        values=values_pred,\n",
    "        old_logprobs=old_logprobs,\n",
    "        old_values=old_values,\n",
    "        advantages=advantages,\n",
    "        returns=returns,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    return loss, old_rewards[:,-1].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8c337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt_pipeline,\n",
    "            tokenizer,\n",
    "            chunk_size = 128):\n",
    "        \n",
    "        self.prompt_pipeline = prompt_pipeline\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        self.prompt_pipeline_loader = self.prompt_pipeline.create_loader(self.chunk_size, shuffle=True)\n",
    "        self.prompt_pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "\n",
    "        self.ref_model = Agent(config.model.model_path)\n",
    "        self.ref_model_device = config.train.ref_model_device\n",
    "        self.ref_model = self.ref_model.to(self.ref_model_device)\n",
    "        \n",
    "        self.tokenizer = tokenizer        \n",
    "    \n",
    "\n",
    "    def make_experience(self, model, num_rollouts = 128):\n",
    "        model_device = next(model.parameters()).device\n",
    "        \n",
    "        ppo_rl_elements = []\n",
    "        while len(ppo_rl_elements) < num_rollouts:\n",
    "            try:\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            except StopIteration:\n",
    "                self.pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            \n",
    "            trajectories = generate(model, self.tokenizer, **batch.to(model_device))\n",
    "\n",
    "            query_tensors = batch.input_ids\n",
    "            response_tensors = trajectories[:, query_tensors.shape[1] :]\n",
    "\n",
    "            all_tokens, attention_mask, position_ids = get_model_inputs(\n",
    "                query_tensors.to(response_tensors.device), response_tensors, self.tokenizer.pad_token_id)\n",
    "            with torch.no_grad():\n",
    "                logits, values = model(\n",
    "                    all_tokens, \n",
    "                    attention_mask=attention_mask, \n",
    "                    position_ids=position_ids)\n",
    "                ref_logits, _ = self.ref_model(\n",
    "                    all_tokens.to(self.ref_model_device),\n",
    "                    attention_mask=attention_mask.to(self.ref_model_device),\n",
    "                    position_ids=position_ids.to(self.ref_model_device))\n",
    "            \n",
    "            all_tokens = all_tokens.cpu()\n",
    "            logits = logits.cpu()\n",
    "            ref_logits = ref_logits.cpu()\n",
    "\n",
    "            logprobs = logprobs_from_logits(logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            ref_logprobs = logprobs_from_logits(ref_logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            \n",
    "            n = trajectories.shape[0]\n",
    "            values = values.cpu()[:, :-1]\n",
    "            query_tensors = query_tensors.cpu()\n",
    "            response_tensors = response_tensors.cpu()\n",
    "            \n",
    "            start = query_tensors.shape[1] - 1\n",
    "            ends = start + attention_mask[:, start:].sum(1)\n",
    "            all_values = [values[i, start : ends[i]] for i in range(n)]\n",
    "            all_logprobs = [logprobs[i, start : ends[i]] for i in range(n)]\n",
    "            \n",
    "            texts = self.tokenizer.batch_decode(trajectories, skip_special_tokens=True)\n",
    "            scores = torch.tensor(reward_fn(texts), device='cpu', dtype=torch.float)\n",
    "\n",
    "            rewards = -config.method.kl_coef * (logprobs - ref_logprobs)\n",
    "            all_rewards = [None] * n\n",
    "            for i in range(n):\n",
    "                rs = rewards[i][start : ends[i]]\n",
    "                rs[-1] = scores[i]\n",
    "                all_rewards[i] = rs\n",
    "            \n",
    "            new_ppo_rl_elements = [\n",
    "                PPORLElement(\n",
    "                    query_tensor=query_tensors[i],\n",
    "                    response_tensor=response_tensors[i],\n",
    "                    logprobs=all_logprobs[i],\n",
    "                    values=all_values[i],\n",
    "                    rewards=all_rewards[i],\n",
    "                )\n",
    "                for i in range(n)\n",
    "            ]\n",
    "\n",
    "            ppo_rl_elements += new_ppo_rl_elements\n",
    "\n",
    "        return ppo_rl_elements, scores.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f50e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, input_ids, attention_mask=None, **kwargs):\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        config.method.gen_kwargs,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    kwargs = dict(generate_kwargs, **kwargs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_results = model.generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    return generated_results\n",
    "\n",
    "\n",
    "def get_model_inputs(query_tensors, response_tensors, pad_token_id):\n",
    "    tokens = torch.cat((query_tensors, response_tensors), dim=1)[:, -config.train.seq_length :]\n",
    "    attention_mask = (tokens.not_equal(pad_token_id).long().to(tokens.device))\n",
    "    position_ids = attention_mask.cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask.eq(0), 0)\n",
    "    return tokens, attention_mask, position_ids\n",
    "\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs_labels = torch.gather(logprobs, dim=-1, index=labels.unsqueeze(-1))\n",
    "    return logprobs_labels.squeeze(-1)\n",
    "\n",
    "\n",
    "def freeze_bottom_causal_layers(model: nn.Module, num_layers_unfrozen: int = 0):\n",
    "    hidden_layers = model.transformer.h\n",
    "    if num_layers_unfrozen == 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)\n",
    "    elif num_layers_unfrozen > 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)[:-num_layers_unfrozen]\n",
    "    else:\n",
    "        hidden_layers_to_freeze = []\n",
    "    for layer in hidden_layers_to_freeze:\n",
    "        layer.requires_grad_(False)\n",
    "\n",
    "        \n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, model_path, num_layers_unfrozen=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = transformers.AutoModelForCausalLM.from_pretrained(model_path, cache_dir=\"./models\")\n",
    "\n",
    "        self.logit_head = self.base_model.get_output_embeddings()\n",
    "        \n",
    "        n_embd = self.base_model.lm_head.in_features\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*2, 1))\n",
    "        \n",
    "        freeze_bottom_causal_layers(self.base_model, num_layers_unfrozen)\n",
    "        \n",
    "    \n",
    "    def generate(self, input_ids, **x):\n",
    "        return self.base_model.generate(input_ids, **x)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "\n",
    "        transformer_outputs = self.base_model.transformer(input_ids=input_ids,\n",
    "                                                          attention_mask=attention_mask,\n",
    "                                                          position_ids=position_ids)\n",
    "    \n",
    "        last_hidden_state = transformer_outputs.last_hidden_state\n",
    "        lm_logits = self.logit_head(last_hidden_state)\n",
    "        value = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        \n",
    "        return lm_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79835e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_fn = pipeline(\n",
    "    model = \"lvwerra/distilbert-imdb\",\n",
    "    top_k=2,\n",
    "    batch_size=config.method.num_rollouts,\n",
    "    device=config.train.reward_model_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e696db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_score(scores):\n",
    "    return dict(map(lambda x: tuple(x.values()), scores))[\"POSITIVE\"]\n",
    "\n",
    "def reward_fn(samples: List[str]) -> List[float]:\n",
    "    sentiments = list(map(get_positive_score, sentiment_fn(samples)))\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180883d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\", split=\"train+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51ad4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\" \".join(review.split()[:config.method.prompt_size]) for review in imdb[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1554f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_ida = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "pad_token_id = 50256\n",
    "\n",
    "max_prompt_length = (config.train.seq_length - config.method.gen_kwargs[\"max_new_tokens\"])\n",
    "test_prompt_pipeline = PromptPipeline(prompts, max_prompt_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f0ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(config.model.model_path, config.model.num_layers_unfrozen).to(config.train.model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c211cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbba22bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(model, tokenizer, input_ids.to(model_device), max_new_tokens=config.method.gen_kwargs[\"max_new_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "284f1b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my feeling about the movie, as well as the style and tone of the famous \"Amadeus\" music films. More than anything, almost the whole trip takes place in New York and more than anything, I felt RUUL UP FRIENDLY was my favorite once. The Pigman song is classic movie music with traces',\n",
       " 'this is one of the stories I\\'m aware of that had a whole lot of potential to make a good film... :/ ). but simply can\\'t make a movie like this. nope. not really an original story but stick with the plot since it seems \"what\" we have is pure crap. nothing',\n",
       " \"I can tell with certainty from this film that this is a terrifying film indeed. From what I hear the level of hype around this film is two times more shocking than Ratatouille. It is disappointing at Church & Mission where the band sottily meets a climactic performance by Travis, it wasn't even worth it\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1058de6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9874017834663391, 0.005386511329561472, 0.0351937897503376]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_fn(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63696263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c93f80bbf0c4b688000a1b1f48848f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "prompt_pipeline = PromptPipeline(prompts, config.train.seq_length, tokenizer)\n",
    "\n",
    "actor = Actor(prompt_pipeline, tokenizer, chunk_size=config.method.chunk_size)\n",
    "\n",
    "store = PPORolloutStorage(tokenizer.pad_token_id)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), **config.optimizer.kwargs)\n",
    "scheduler = CosineAnnealingLR(opt, **config.scheduler.kwargs)\n",
    "\n",
    "n_updates_per_batch = config.method.ppo_epochs\n",
    "total_steps = 400 # TODO: fix this\n",
    "\n",
    "tbar = tqdm(initial=0, total=total_steps)\n",
    "\n",
    "for _ in range(config.train.epochs):\n",
    "    \n",
    "    store.clear_history()\n",
    "    rollouts, score = actor.make_experience(model, config.method.num_rollouts)\n",
    "    store.push(rollouts)\n",
    "    train_dataloader = store.create_loader(config.train.batch_size, shuffle=True)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        for _ in range(n_updates_per_batch):\n",
    "\n",
    "            loss, reward = loss_fn(batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            tbar.update()\n",
    "    \n",
    "    tbar.set_description(f\"| score: {score:.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "577f8e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af00680",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my feeling about the movie was to see it delivered, which is actually brilliant.<br /><br />Everything worked brilliantly.\n",
      " 0.9942626357078552\n",
      "this is good Horror. good idea. the premise is great in this title. overall good. the writing is good fun. indeed the best elements are definitely good. have fun.\n",
      " 0.9921297430992126\n",
      "I can tell with certainty that her photography style is fantastic and well worth the time.<br /><br />This was my first delight.\n",
      " 0.9941641688346863\n",
      "all rewards mean: 0.9935188492139181\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(\n",
    "    model,       \n",
    "    tokenizer,\n",
    "    input_ids.to(model_device),\n",
    "#     min_length=20,\n",
    "#     max_new_tokens=100,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.92, \n",
    "#     top_k=0\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "rewards = reward_fn(generated_text)\n",
    "print(generated_text[0].replace('\\n', ' ') + '\\n', rewards[0])\n",
    "print(generated_text[1].replace('\\n', ' ') + '\\n', rewards[1])\n",
    "print(generated_text[2].replace('\\n', ' ') + '\\n', rewards[2])\n",
    "print('all rewards mean:',np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a86d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
