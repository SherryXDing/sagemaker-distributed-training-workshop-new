{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6a79ff-2820-4cc6-91dd-7863e2be0cc3",
   "metadata": {},
   "source": [
    "# Train your own reward model with PyTorch and Hugging Face locally on SageMaker Studio Notebooks\n",
    "In this notebook we will use the IMDB dataset to train a reward model that provides a higher score for text which humans have labelled as positive, and a lower score for the negative text. This implements a new training loop for the reward training in PyTorch, pointing to a base model from Hugging Face. We then use this model on test data to sort new samples into positive and negative sentiment, achieving a 97% success rate. \n",
    "\n",
    "You can use this notebook with the IMDB dataset as provided, or you can use it to slightly modify a new dataset. \n",
    "\n",
    "This notebook will likely take a few hours to run as it is today. Please use an instance with at least a few accelerators, such as an ml.g5.12xlarge. You'll also need a kernel with at least Python 3.8, we the latest base Python  kernel in SageMaker Studio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea5aca-59d2-4ff1-817b-35db9cc6236d",
   "metadata": {},
   "source": [
    "### Step 0. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d6a7d-40d1-471e-aed5-765f1c3310b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "bitsandbytes\n",
    "git+https://github.com/huggingface/transformers.git\n",
    "git+https://github.com/huggingface/peft.git\n",
    "datasets\n",
    "scipy\n",
    "omegaconf \n",
    "scikit-learn \n",
    "sentencepiece \n",
    "protobuf==3.20.3\n",
    "einops \n",
    "evaluate \n",
    "omegaconf \n",
    "tensorboard \n",
    "torchtyping \n",
    "matplotlib \n",
    "cchardet \n",
    "chardet\n",
    "numpy\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777f215-1fd6-4dd6-ba44-f16affbafa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef93b9b-e0f5-46b4-92d6-ff58bb08ad5e",
   "metadata": {},
   "source": [
    "Now restart your kernel and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596b8b1",
   "metadata": {},
   "source": [
    "### Step 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58923696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7462ca",
   "metadata": {},
   "source": [
    "### Step 2. Initiatlize settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10ff84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selected_train_samples = 1000\n",
    "\n",
    "args = {\n",
    "    \"seed\": 42,\n",
    "    # change the model name here \n",
    "    'model_name_or_path': 'facebook/opt-1.3b',\n",
    "    'learning_rate': 5e-5,\n",
    "    'batch_size': 2,\n",
    "    'gradient_accumulation_steps': 32,\n",
    "    'num_train_epochs': 1,\n",
    "    'num_workers': 10,\n",
    "    'seq_length': 1024,\n",
    "    'logging_steps': 10,\n",
    "}\n",
    "\n",
    "args = DictConfig(args)\n",
    "\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a791d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59209c1b",
   "metadata": {},
   "source": [
    "### Step 3. Data Preparation\n",
    "\n",
    "- Use the following cell if your dataset is already in the RLHF appropriate format (for example `Anthropic/hh-rlhf` which has `chosen` and `rejected` columns).\n",
    "- Or follow the second cell to create a custom dataset pairing positive and negative samples according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f8884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "# train_dataset = raw_dataset['train']\n",
    "\n",
    "\n",
    "# def tokenize_fn(text, max_length=args.seq_length):\n",
    "#     encoded = tokenizer(\n",
    "#         text,\n",
    "#         padding='max_length',\n",
    "#         max_length=max_length,\n",
    "#         truncation=True,\n",
    "#         add_special_tokens=False,\n",
    "#     )\n",
    "#     return encoded\n",
    "\n",
    "\n",
    "# def encode(sample):\n",
    "#     chosen_encoded = tokenize_fn(sample['chosen'])\n",
    "#     rejected_encoded = tokenize_fn(sample['rejected'])\n",
    "#     encoded = {\n",
    "#         'chosen_input_ids':chosen_encoded['input_ids'],\n",
    "#         'chosen_attention_mask':chosen_encoded['attention_mask'],\n",
    "#         'rejected_input_ids':rejected_encoded['input_ids'],\n",
    "#         'rejected_attention_mask':rejected_encoded['attention_mask'],\n",
    "#     }\n",
    "#     return encoded\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.shuffle().map(encode, num_proc=args.num_workers)\n",
    "\n",
    "# train_dataset = train_dataset.with_format(\"torch\")\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1c3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_dataset(raw_dataset):\n",
    "    df = raw_dataset.to_pandas()\n",
    "    negative_df = df[df['label']==0]\n",
    "    positive_df = df[df['label']==1]\n",
    "    negative_df = negative_df.drop(\n",
    "        columns=['label']).rename(\n",
    "        columns={'text': 'rejected'})\n",
    "    # shuffle the data\n",
    "    positive_df = positive_df.sample(\n",
    "        frac=1, random_state=0).reset_index(\n",
    "        drop=True).drop(columns=['label']).rename(\n",
    "        columns={'text': 'chosen'})\n",
    "    joined_df = negative_df.join(positive_df)\n",
    "\n",
    "    def tokenize_fn(texts, max_length=args.seq_length):\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        return encoded\n",
    "\n",
    "    rejected_encoded = tokenize_fn(joined_df.rejected.values.tolist())\n",
    "    joined_df['rejected_input_ids'] = rejected_encoded['input_ids']\n",
    "    joined_df['rejected_attention_mask'] = rejected_encoded['attention_mask']\n",
    "    encoded_chosen = tokenize_fn(joined_df.chosen.values.tolist())\n",
    "    joined_df['chosen_input_ids'] = encoded_chosen['input_ids']\n",
    "    joined_df['chosen_attention_mask'] = encoded_chosen['attention_mask']\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(joined_df, preserve_index=False)\n",
    "    \n",
    "    return train_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1786c4-36a4-4983-a849-e12ce30a518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"imdb\")\n",
    "raw_train_dataset = raw_dataset['train']\n",
    "    \n",
    "train_dataset = create_custom_dataset(raw_train_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adf281",
   "metadata": {},
   "source": [
    "### Step 4. Load your base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20650643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bf2e4",
   "metadata": {},
   "source": [
    "### Step 5. Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7539a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422c5bd4818845e392ef7515a2f637ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 1\n",
    "print_interval=args.logging_steps\n",
    "num_batches = len(train_dataloader)\n",
    "# progress_bar = tqdm(total=num_batches*args.num_train_epochs, leave=True)\n",
    "# progress_bar.set_description(f\"| Train: Epoch {epoch}, evaluating ... |\")\n",
    "all_losses = []\n",
    "i = 0\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(1, args.num_train_epochs+1):\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        chosen_input_ids = batch['chosen_input_ids'].to(model.device)\n",
    "        chosen_attention_mask = batch['chosen_attention_mask'].to(model.device)\n",
    "        rejected_input_ids = batch['rejected_input_ids'].to(model.device)\n",
    "        rejected_attention_mask = batch['rejected_attention_mask'].to(model.device)\n",
    "\n",
    "        r_w = model(chosen_input_ids, chosen_attention_mask).logits\n",
    "        r_l = model(rejected_input_ids, rejected_attention_mask).logits\n",
    "\n",
    "        loss = -F.logsigmoid(r_w - r_l).mean()\n",
    "\n",
    "        # Accumulate the gradients\n",
    "        loss /= args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (i + 1) % args.gradient_accumulation_steps == 0 or i + 1 == len(train_dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        all_losses.append( loss.item() )\n",
    "        \n",
    "        print(loss.item() )\n",
    "\n",
    "\n",
    "        # if i%print_interval==0:\n",
    "        #     progress_bar.set_description(f\"| Train: Epoch {epoch}, loss = {loss.item():4f} |\")\n",
    "        #     progress_bar.refresh()\n",
    "        # progress_bar.update()\n",
    "        # i+=1\n",
    "\n",
    "# progress_bar.set_description(f\"| Train: Epoch {epoch}, loss = {loss.item():4f} |\")\n",
    "# progress_bar.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dee3c",
   "metadata": {},
   "source": [
    "### Step 6. Evaluate your reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a50634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18bcd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = raw_dataset['test']\n",
    "\n",
    "# test_dataset = test_dataset.map(encode, num_proc=args.num_workers)\n",
    "\n",
    "# test_dataset = test_dataset.with_format(\"torch\")\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a72aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_dataset = raw_dataset['test']\n",
    "    \n",
    "test_dataset = create_custom_dataset(raw_test_dataset)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "268901b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd60aadc4084ddaadb786c0dad383c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of orders after training:  0.97\n"
     ]
    }
   ],
   "source": [
    "num_correct_orders = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch in tqdm(test_dataloader):\n",
    "\n",
    "        chosen_input_ids = batch['chosen_input_ids'].to(model.device)\n",
    "        chosen_attention_mask = batch['chosen_attention_mask'].to(model.device)\n",
    "        rejected_input_ids = batch['rejected_input_ids'].to(model.device)\n",
    "        rejected_attention_mask = batch['rejected_attention_mask'].to(model.device)\n",
    "\n",
    "        r_w = model(chosen_input_ids, chosen_attention_mask).logits\n",
    "        r_l = model(rejected_input_ids, rejected_attention_mask).logits\n",
    "\n",
    "        num_correct_orders += (r_w - r_l>0).sum().item()\n",
    "        \n",
    "print('Accuracy of orders after training: ', num_correct_orders/(len(test_dataloader)*args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671b4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2105f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lora]",
   "language": "python",
   "name": "conda-env-lora-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
