{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f671ae67-f9fe-4f36-a40c-4911a50504d8",
   "metadata": {},
   "source": [
    "# Text to Image generation on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c5040-dfde-4a3f-ab62-91d107b28c5c",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how you can fine-tune an existing Stable Diffusion model on SageMaker and deploy it for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba77a89-0318-41af-9f1c-b9e54a543904",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40395fdd-0087-43e2-a8e5-d2e482ccf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75580a08-54ce-4f55-a7ea-b34eaf1d6fc9",
   "metadata": {},
   "source": [
    "This notebook is purely educational for showing how to fine-tune latent-stable-diffusion on Amazon SageMaker. Neither the images produced or code represent Amazon or its views in any way shape or form. To properly leverage this codebase, read the corresponding licenses from [CompVis](https://huggingface.co/spaces/CompVis/stable-diffusion-license) (the model) and [Conceptual Captions](https://huggingface.co/datasets/conceptual_captions) (from Google, but you will use HF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cb30c-da52-4210-b12a-5b75c9c34eb5",
   "metadata": {},
   "source": [
    "This demo requires a g4dn.12xlarge or more powerful instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600caeaf-5e86-4431-a042-a9beb4600eed",
   "metadata": {},
   "source": [
    "Model weights were provided by CompVis/stable-diffusion-v1-4. You can find the licensing, README and more [here](https://huggingface.co/CompVis/stable-diffusion-v1-4). To download the weights, you will need to have a huggingface account, accept the terms on the aforementioned link, then generate your user authenticated token. These steps are beyond the scope of this Notebook. Please note that the finetune.py script has been slightly modified from a PR request [here](https://github.com/huggingface/diffusers/pull/356)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a266e-87ae-498c-8bde-7a180592ddd3",
   "metadata": {},
   "source": [
    "You will install some libraries so that you can use stable-diffusion locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb252eb-2f7f-46af-a2e7-f9867c98a645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install diffusers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04881b-c7bb-4edc-84d2-d1befd3dff8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.21.0 -q\n",
    "!pip install ftfy spacy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870350f-5d95-4dba-a008-6ed946b18ac1",
   "metadata": {},
   "source": [
    "## 1. Download Model and Data\n",
    "Now you will download the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2320549-3ef6-4b95-b011-2a80e79f0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "token_value = 'INSERT TOKEN HERE'\n",
    "force = False\n",
    "if os.path.exists('sd-base-model') and (not force):\n",
    "    d = './sd-base-model/'\n",
    "else:\n",
    "    d = \"CompVis/stable-diffusion-v1-4\"\n",
    "    \n",
    "model = DiffusionPipeline.from_pretrained(d, cache_dir=os.path.join(os.getcwd(),'base-model'),use_auth_token=token_value)\n",
    "\n",
    "if d == \"CompVis/stable-diffusion-v1-4\":\n",
    "    d = './sd-base-model'\n",
    "    !rm -rf sd-base-model\n",
    "    model.save_pretrained('./sd-base-model/')\n",
    "    !rm -rf ./base-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e208e09-f46d-4ce4-a65d-64128980cd35",
   "metadata": {},
   "source": [
    "And the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f37ab-3a84-45d0-903b-82357e3d1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3392d-7067-4dbd-a80e-29da18b617e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_name = 'conceptual_captions'\n",
    "!rm -rf conceptual_captions\n",
    "dataset = load_dataset(\"conceptual_captions\")\n",
    "!mkdir {data_name}\n",
    "!cp -r ./sd-base-model {data_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172db9f-0e7f-4a71-98c2-d6227ba042ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to visualize the dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a13745-3d3d-4ee0-b9bd-209858b5c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe2eec-836d-4e38-8674-5d1fed4428cc",
   "metadata": {},
   "source": [
    "This following cell will allow you to download the images (not provided in the previous download), and extract a subset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15959fe-2011-446f-9594-4bed070cb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "df = dataset['train'].to_pandas()\n",
    "def download_file(url,index):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url,f'./{data_name}/{index}.jpg')\n",
    "j = 0\n",
    "indexes = []\n",
    "images = set()\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.loc[i,'sm_key'] = f'/opt/ml/input/data/training/{j}.jpg'\n",
    "        df.loc[i,'local_key'] = f'{j}.jpg'\n",
    "        download_file(df.loc[i,'image_url'],j)\n",
    "        img = Image.open(f'./conceptual_captions/{j}.jpg')\n",
    "        j += 1\n",
    "        indexes.append(i)\n",
    "    except Exception as e:\n",
    "        print(f\"file didn't download will continue {i}\")\n",
    "        print(e)\n",
    "            \n",
    "    if (j % 100 == 0) and (j>0):#You can change this to train on a larger dataset\n",
    "        break\n",
    "df = df.iloc[indexes,:]\n",
    "df.to_parquet(f'./{data_name}/dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84166645-29d5-4835-887a-c9ad942216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again feel free to run the following two cells to visualize a sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ea4c0-a3b6-455f-b137-acfed9013249",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_dataset('parquet',data_dir=f'./{data_name}',data_files='dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132363e1-c734-45a5-9fce-3e432fafe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "img,text = d['train'][idx]['local_key'],d['train'][idx]['caption']\n",
    "print(img)\n",
    "print(text)\n",
    "Image.open(f'./{data_name}/{img}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962a94b-bf13-4154-8973-a9fe8f30e7c3",
   "metadata": {},
   "source": [
    "Additionally, the data you will be using comes from mscoco. However, you can also download from [here](https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT) which uses the dataset from [here](https://academictorrents.com/details/74dec1dd21ae4994dfd9069f9cb0443eb960c962). Then use this [link](https://github.com/rom1504/img2dataset) to quickly fill in the datasets files. For the purpose of this notebook you can download a few samples using the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6c915-3ef8-4433-8628-b85c799a9873",
   "metadata": {},
   "source": [
    "# 2. Training\n",
    "You will use distributed training, to do so you need to leverage any existing GPU's. The first cell will evaluate to see how many gpus are on the current system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fda500-6894-4e5f-acbc-5179d354d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "processes_per_host = subprocess.Popen(\"nvidia-smi -q | awk '/Attached GPUs/ {print $4}'\",\n",
    "                              shell=True,\n",
    "                              stdout=subprocess.PIPE)\n",
    "processes_per_host = int(processes_per_host.stdout.read().decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e54d1a-68f1-4d31-b3c6-66a6d69af5df",
   "metadata": {},
   "source": [
    "The following cell will enable you to build an estimator for training locally, and fit on the local dataset you previously built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f57b0-3355-4098-8d81-db9fcabba232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "est = HuggingFace(\n",
    "    entry_point='finetune.py',\n",
    "    source_dir='src',\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker',\n",
    "    #pytorch_version=\"1.10.2\",\n",
    "    #transformers_version=\"4.17.0\",\n",
    "    sagemaker_session=LocalSession(),\n",
    "    role=get_execution_role(),\n",
    "    instance_type='local_gpu',\n",
    "    output_path='file://{}'.format(os.path.join(os.getcwd(),'model')),\n",
    "    py_version='py38',\n",
    "    base_job_name='test',\n",
    "    instance_count=1,\n",
    "    hyperparameters={\n",
    "        'pretrained_model_name_or_path':'/opt/ml/input/data/training/sd-base-model',\n",
    "        'dataset_name':'/opt/ml/input/data/training/dataset.parquet',\n",
    "        'caption_column':'caption',\n",
    "        'image_column':'sm_key',\n",
    "        'resolution':256,\n",
    "        'mixed_precision':'fp16',\n",
    "        'train_batch_size':2,\n",
    "        'learning_rate': '1e-10',\n",
    "        'max_train_steps':100,\n",
    "        'num_train_epochs':1,\n",
    "        'output_dir':'/opt/ml/model/sd-output-final',   \n",
    "    },\n",
    "    distribution={\"mpi\":{\"enabled\":True,\"processes_per_host\":processes_per_host}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f3cb5-cf20-4320-beec-95dc17696737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note training can take upwards of 25 minutes (13 minutes for saving the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1bc59-94c2-4da5-bf86-2db36acd1985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "est.fit(f'file://./{data_name}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055782b-b410-4697-aff4-34546a066261",
   "metadata": {},
   "source": [
    "The \"Aborting on container exit\" line may hang for up to 15 minutes due to the size of the model being compressed, saved, and uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf68d0-4c25-45c0-a4a5-3326cc939f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est.model_data) #In case you have to restart kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e099d-c6e0-463c-8310-e70576af5651",
   "metadata": {},
   "source": [
    "## 3. Inference\n",
    "Prior to doing inference you will need to extand an existing Deep Learning Container. Feel free to look at Dockerfile-Inf under the src directory for more details on this file. Otherwise, this following cell will build a local container for use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249abf3-4cb3-407c-b60f-10e657f57efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!DOCKER_BUILDKIT=1 docker build ./src -f ./src/Dockerfile-Inf -t local:latest -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f572b-238c-4f13-8a6b-255f6c249890",
   "metadata": {},
   "source": [
    "Define your Model for deployment (This can be skipped due to the previous train job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a94a6-5d9d-43f5-9d3f-a9c788ae3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "#This cell could be done separately, and you could deploy the following one directly.\n",
    "## However, if you happen to restart later, this cell will run but you will need to input\n",
    "##  est.model_data into model_data =\n",
    "est=HuggingFaceModel(role=get_execution_role(),\n",
    "                     py_version='py38',\n",
    "                      model_data=est.model_data,\n",
    "                      image_uri='local:latest',\n",
    "                      sagemaker_session=LocalSession(),\n",
    "                      model_server_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc9aad-3cac-450b-b3f1-f349425b5e2e",
   "metadata": {},
   "source": [
    "Deploy your model for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc523a5c-a6ea-4471-aac5-b46358781ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = est.deploy(instance_type='local_gpu',\n",
    "                  initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c6e1e-d072-441f-8643-fda77ee09ee2",
   "metadata": {},
   "source": [
    "Provide prompts for training. The first text argument is based on this current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df953476-7ea4-49a7-9d2b-056125cbb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [text,'A photo of an astronaut riding a horse on mars', \n",
    "           'A dragonfruit wearing karate belt in the snow.', \n",
    "           'Teddy bear swimming at the Olympics 400m Butter-fly event.',\n",
    "           'A cute sloth holding a small glowing treasure chest.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75b580-256b-4306-8ebd-3199829b6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e555b6-abae-4938-bef8-4cef75b61bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = [pred.predict({'inputs':prompt}) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1d934-5d93-4527-be55-e69a0eebe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [output['images'][0] for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b33055-9f20-461d-ac6f-cb8d5db72f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(out):\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    return Image.open(BytesIO(base64.b64decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbaf8-033f-4ac2-82d5-69f672e182f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [[process_result(output),prompt] for output,prompt in zip(outputs,prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc44f86-2a55-4e50-8045-b53775746c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results from the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20428b6-19dc-4aca-931d-a4f7d0460722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.figure()\n",
    "    plt.title(images[i][1])\n",
    "    plt.imshow(images[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29c9c1-137a-4246-88bc-0d2e63ede648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up your endpoint\n",
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477b87c-8d8b-4987-b7ab-2cdf67031bcf",
   "metadata": {},
   "source": [
    "## 4. (Bonus) compare against the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459563fb-df31-4eae-bcfc-d9fa2f07bb09",
   "metadata": {},
   "source": [
    "Using the previous model that you defined as a base model, you can also evaluate it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356163a-d480-4fe8-ba6e-d6de8e98cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c6d9f-a626-4594-9d37-344cbd0ba1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = [model(prompt) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7785014-6a78-458a-9d4d-c6822a1dc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3247112-fb99-4bc6-acbc-12a44fcc963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outs)):\n",
    "    plt.figure()\n",
    "    plt.title(prompts[i])\n",
    "    plt.imshow(outs[i].images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011ff20-fa57-407e-8299-bd34920f383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will need to reset the kernel to remove the model from the GPU Memory if you wish to train more locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e4934-09b4-4195-ac30-c6dc295c7790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
