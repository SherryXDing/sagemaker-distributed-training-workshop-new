{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f671ae67-f9fe-4f36-a40c-4911a50504d8",
   "metadata": {},
   "source": [
    "# Text to Image generation on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c5040-dfde-4a3f-ab62-91d107b28c5c",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how you can fine-tune an existing Stable Diffusion model on SageMaker and deploy it for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba77a89-0318-41af-9f1c-b9e54a543904",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426de663-3dd0-446a-a0f5-8517d3727ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import torch\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40395fdd-0087-43e2-a8e5-d2e482ccf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75580a08-54ce-4f55-a7ea-b34eaf1d6fc9",
   "metadata": {},
   "source": [
    "This notebook is purely educational for showing how to fine-tune latent-stable-diffusion on Amazon SageMaker. Neither the images produced or code represent Amazon or its views in any way shape or form. To properly leverage this codebase, read the corresponding licenses from [CompVis](https://huggingface.co/spaces/CompVis/stable-diffusion-license) (the model) and [Conceptual Captions](https://huggingface.co/datasets/conceptual_captions) (from Google, but you will use HF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cb30c-da52-4210-b12a-5b75c9c34eb5",
   "metadata": {},
   "source": [
    "This demo requires a g5.12xlarge or more powerful instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600caeaf-5e86-4431-a042-a9beb4600eed",
   "metadata": {},
   "source": [
    "Model weights were provided by CompVis/stable-diffusion-v1-4. You can find the licensing, README and more [here](https://huggingface.co/CompVis/stable-diffusion-v1-4). To download the weights, you will need to have a huggingface account, accept the terms on the aforementioned link, then generate your user authenticated token. These steps are beyond the scope of this Notebook. Please note that the finetune.py script has been slightly modified from a PR request [here](https://github.com/huggingface/diffusers/pull/356)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a266e-87ae-498c-8bde-7a180592ddd3",
   "metadata": {},
   "source": [
    "You will install some libraries so that you can use stable-diffusion locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb252eb-2f7f-46af-a2e7-f9867c98a645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r ./src/requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870350f-5d95-4dba-a008-6ed946b18ac1",
   "metadata": {},
   "source": [
    "## 1. Download Model and Data\n",
    "Now you will download the model first. You can modify the following cell if you want an example with your own Token. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e6eff5d-455b-4497-9983-63d833523e36",
   "metadata": {},
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "token_value = 'INSERT TOKEN HERE'\n",
    "force = False\n",
    "d = \"CompVis/stable-diffusion-v1-4\"\n",
    "cached = os.path.join(os.getcwd(),'base-model')\n",
    "    \n",
    "model = DiffusionPipeline.from_pretrained(d, cache_dir=cached,use_auth_token=token_value)\n",
    "d = './sd-base-model'\n",
    "!rm -rf sd-base-model\n",
    "model.save_pretrained('./sd-base-model/')\n",
    "!rm -rf ./base-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738e5c8-6b8f-4d7f-b356-2923385f9e65",
   "metadata": {},
   "source": [
    "Otherwise, you can download the data from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f446849-ea83-4d8a-822d-b75f54cc63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"INSERT BUCKET NAME\"\n",
    "path = \"conceptual_captions\"\n",
    "s3_train_channel = f\"s3://{bucket}/{path}\"\n",
    "image = '0.jpg'\n",
    "image_file = f\"./dta/{image}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5b9f2-b651-421e-b13c-4a382852db2a",
   "metadata": {},
   "source": [
    "Or if you would like to use the original dataset from huggingface, you can download the parquet file using the following code, and then download the images independently"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65860946-497d-4c75-a3ce-1b3ba89ee1c0",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset('conceptual_captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14188fb-2e5d-4ac1-aa13-d452a78e0370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p dta\n",
    "!aws s3 cp {s3_train_channel}/{image} ./dta/\n",
    "!aws s3 cp {s3_train_channel}/dataset.parquet ./dta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4db69-8b8c-4520-a623-02ef6287ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./dta/dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4862f17-4f0a-4b2d-af8e-1ab628eb9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64398d0a-d96c-49e3-aed0-222f3ad4bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = df['caption'][0]\n",
    "from PIL import Image\n",
    "\n",
    "print (caption)\n",
    "\n",
    "Image.open(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f83514-0d3f-4e8b-8b11-59d8a3707def",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962a94b-bf13-4154-8973-a9fe8f30e7c3",
   "metadata": {},
   "source": [
    "Additionally, the data you will be using comes from mscoco. However, you can also download from [here](https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT) which uses the dataset from [here](https://academictorrents.com/details/74dec1dd21ae4994dfd9069f9cb0443eb960c962). Then use this [link](https://github.com/rom1504/img2dataset) to quickly fill in the datasets files. For the purpose of this notebook you can download a few samples using the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6c915-3ef8-4433-8628-b85c799a9873",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Training\n",
    "You will use distributed training, to do so you need to leverage any existing GPU's. The first cell will evaluate to see how many gpus are on the current system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9d8a5-91ba-4c3f-8703-8cb21e338f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a6baa-49f1-49f1-b09f-4e46f99291ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f05777-e2a5-480d-a16f-56e95500320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "output = None\n",
    "def get_processes_per_host(instance_type):\n",
    "    if instance_type == 'ml.g5.12xlarge':\n",
    "        processes_per_host = 4\n",
    "    elif 'local' in instance_type:\n",
    "        from torch import cuda\n",
    "        processes_per_host = cuda.device_count()\n",
    "        local = True\n",
    "    else:\n",
    "        print ('Please look up the number of GPUs per node from the EC2 page here: https://aws.amazon.com/ec2/instance-types/ ')\n",
    "    \n",
    "    return processes_per_host\n",
    "\n",
    "\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "processes_per_host = get_processes_per_host(instance_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e54d1a-68f1-4d31-b3c6-66a6d69af5df",
   "metadata": {},
   "source": [
    "The following cell will enable you to build an estimator for training locally, and fit on the local dataset you previously built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c37ae3-fd6e-46e6-837f-ac7a33d8943d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# If you want to train locally you will need to run the following \n",
    "if local :\n",
    "    !./process.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f57b0-3355-4098-8d81-db9fcabba232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "\n",
    "\n",
    "est = HuggingFace(\n",
    "    entry_point='finetune.py',\n",
    "    source_dir='src',\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com' + \n",
    "     '/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker',\n",
    "    sagemaker_session=Session() if 'local' not in instance_type else LocalSession(boto_session=LocalSession().boto_session),\n",
    "    role=get_execution_role(),\n",
    "    instance_type=instance_type,\n",
    "    keep_alive_time_in_seconds = 28800,\n",
    "    # output_path= can define s3 output here,\n",
    "    py_version='py38',\n",
    "    base_job_name='stable-diffusion',\n",
    "    instance_count=1,\n",
    "    # all opt/ml paths point to SageMaker training \n",
    "    hyperparameters={\n",
    "        'pretrained_model_name_or_path':'/opt/ml/input/data/training/sd-base-model',\n",
    "        'dataset_name':'/opt/ml/input/data/training/dataset.parquet',\n",
    "        'caption_column':'caption',\n",
    "        'image_column':'sm_key',\n",
    "        'resolution':256,\n",
    "        'mixed_precision':'fp16',\n",
    "        'train_batch_size':2,\n",
    "        'learning_rate': '1e-10',\n",
    "        'max_train_steps':100,\n",
    "        'num_train_epochs':1,\n",
    "        'output_dir':'/opt/ml/model/sd-output-final',   \n",
    "    },    \n",
    "    distribution={\"mpi\":{\"enabled\":True,\"processes_per_host\":processes_per_host}},\n",
    "    profiler_config=profiler_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f3cb5-cf20-4320-beec-95dc17696737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note training can take upwards of 25 minutes (13 minutes for saving the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1bc59-94c2-4da5-bf86-2db36acd1985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "est.fit(inputs={'training':s3_train_channel},wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055782b-b410-4697-aff4-34546a066261",
   "metadata": {},
   "source": [
    "The \"Aborting on container exit\" line may hang for up to 20 minutes due to the size of the model being compressed, saved, and uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf68d0-4c25-45c0-a4a5-3326cc939f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(est.model_data) #In case you have to restart kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e099d-c6e0-463c-8310-e70576af5651",
   "metadata": {},
   "source": [
    "## 3. Inference\n",
    "Prior to doing inference you will need to extand an existing Deep Learning Container. Feel free to look at Dockerfile-Inf under the src directory for more details on this file. Otherwise, this following cell will build a local container for use in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e611c3-6539-4791-85a1-89f8ed902e40",
   "metadata": {},
   "source": [
    "Prior to deploying you will need to build your extended image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760be29-3119-4505-a5ee-5747465c9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "#Process and push_to_ecr may take some time to complete\n",
    "if local and (output is None):\n",
    "    !./process.sh\n",
    "!./src/push_to_ecr.sh\n",
    "with open('output.txt','r') as f:\n",
    "    image_uri = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92e558b1-1a11-41c7-8539-c4dbd04319e9",
   "metadata": {},
   "source": [
    "#View the output if you desire.\n",
    "output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f572b-238c-4f13-8a6b-255f6c249890",
   "metadata": {},
   "source": [
    "Define your Model for deployment (This can be skipped due to the previous train job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c027e-a551-4a63-b9d6-6e6aecdac60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "\n",
    "est=HuggingFaceModel(role=get_execution_role(),\n",
    "                     py_version='py38',\n",
    "                     model_data=est.model_data,\n",
    "                     image_uri=image_uri.strip(),\n",
    "                     sagemaker_session=LocalSession() if 'local' in instance_type else Session(),\n",
    "                     model_server_workers= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc9aad-3cac-450b-b3f1-f349425b5e2e",
   "metadata": {},
   "source": [
    "Deploy your model for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc523a5c-a6ea-4471-aac5-b46358781ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = est.deploy(instance_type=instance_type,\n",
    "                  initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c6e1e-d072-441f-8643-fda77ee09ee2",
   "metadata": {},
   "source": [
    "Provide prompts for training. The first text argument is based on this current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df953476-7ea4-49a7-9d2b-056125cbb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [caption,'A photo of an astronaut riding a horse on mars', \n",
    "           'A dragonfruit wearing karate belt in the snow.', \n",
    "           'Teddy bear swimming at the Olympics 400m Butter-fly event.',\n",
    "           'A cute sloth holding a small glowing treasure chest.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef67e8-6054-474d-97ab-fb280ff3874c",
   "metadata": {},
   "source": [
    "For more parameters feel free to explore [here](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion), just add 'parameters':{'key':'value'} to the input dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e555b6-abae-4938-bef8-4cef75b61bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = [pred.predict({'inputs':prompt}) \\\n",
    "           for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1d934-5d93-4527-be55-e69a0eebe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [output['images'][0] for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b33055-9f20-461d-ac6f-cb8d5db72f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(out):\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    return Image.open(BytesIO(base64.b64decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbaf8-033f-4ac2-82d5-69f672e182f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [[process_result(output),prompt] for output,prompt in zip(outputs,prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc44f86-2a55-4e50-8045-b53775746c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results from the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20428b6-19dc-4aca-931d-a4f7d0460722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.figure()\n",
    "    plt.title(images[i][1])\n",
    "    plt.imshow(images[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29c9c1-137a-4246-88bc-0d2e63ede648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up your endpoint\n",
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bd07f-8fbd-4a11-9ce8-810ac1a3d906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
