{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ab76c0-dafa-4b4e-8e60-ec3eba6c8f93",
   "metadata": {},
   "source": [
    "# Hello world SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b93d69-8e12-4471-a9dc-53b2e53d8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5e2edc-3a54-4aff-ad73-50415ed48b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"jumpstart-dft-hf-llm-mixtral-8x7b-i-20240319-180246\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be329e3e-3733-4e96-b52a-b031e12f5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "        \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # modify this lower line for the model, some of them send lists, others send JSON directly\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9384490f-c68c-478e-9f73-f3c53d901949",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are an AI Assistant named Mixtral. You are funny, kind, concise, and witty. \n",
    "You will get a question from your customer below. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Don't be too predictable, mix up your answers.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9193b11d-640d-4856-b91c-825dfa6100b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = SagemakerEndpoint(endpoint_name = endpoint_name,\n",
    "                        region_name = 'us-west-2',\n",
    "                        #hopefully this is \n",
    "                        model_kwargs = { \"max_new_tokens\": 256, \"do_sample\": True},\n",
    "                        # streaming=True,\n",
    "                        content_handler = MixtralHandler()\n",
    "                        # callbacks = [StdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# handler = StdOutCallbackHandler()\n",
    "\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a6b3a4-c221-4610-bbd8-c95e7507c72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Absolutely, Emily! I'm here to assist you. Thanks for asking!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Are you there Mixtral? It's me, Emily.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "664cbd4e-e8c8-4d8e-bd93-243b1ed9cc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Of course, I'd be happy to! Why don't scientists trust atoms? Because they make up everything! Hope that gave you a chuckle.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Great! Good to meet you again. Can you tell me a joke?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec832c-18aa-4360-bf4a-50e970a6e59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12976e45-d882-42b5-bc00-cab34a670005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94679493-9513-4c1c-911b-fbb6c6d5240d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
