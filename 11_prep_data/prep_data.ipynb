{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "babd0546-8662-45d7-b00b-0967c9d878ba",
   "metadata": {},
   "source": [
    "# Prepare labelled Q/A data with SageMaker\n",
    "In this notebook we'll prepare a dataset for supervised fine-tuning with Amazon SageMaker. In particular we'll look at ten recent papers from the Amazon Science community, and send these to a Mechanical Turk workforce using SageMaker Ground Truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359a859-b00f-4655-9dce-b07e71d3b3e3",
   "metadata": {},
   "source": [
    "### Step 1. Download the data\n",
    "First let's take a look at the raw papers data. These have already been converted to raw text files, and they do not have any of the original images. They are already uploaded to an S3 bucket, so let's download them locally and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faddfae3-51b1-4d1d-a5c4-a37d98d47e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750296e8-0444-47ab-a531-c36474107ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3://dist-train/amazon-science/Amazon Science Training Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4657ea28-dc57-4465-80a9-e34d88ab9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync '{s3_path}' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "117b1444-0da7-48fe-9269-27744f29a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five.txt  four.txt  nine.txt  one.txt  seven.txt  ten.txt  three.txt  two.txt\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58ed18-68ae-49c2-8e3d-5d8963ba2e6d",
   "metadata": {},
   "source": [
    "### Step 2. Extract the abstracts\n",
    "Next, let's pull just the abstracts out from these papers. Fortunately these 10 samples all use the same word to indicate the end of the abstract, which is either `Introduction` or the same in all capitals. We'll use that logic to split the data from the paper and grab just the paper title, author names, and abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acb0c3e5-8b0a-4ad4-86fc-3edc8b2f3169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nine.txt\n",
      "1910\n",
      "data/five.txt\n",
      "1797\n",
      "data/ten.txt\n",
      "1284\n",
      "data/four.txt\n",
      "1554\n",
      "data/two.txt\n",
      "1215\n",
      "data/one.txt\n",
      "1425\n",
      "data/seven.txt\n",
      "2046\n",
      "data/three.txt\n",
      "1497\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_abstracts(abs):\n",
    "    for k,v in abs.items():\n",
    "        print (k)\n",
    "        print (v['length'])\n",
    "\n",
    "def papers_etl(local_data_path):\n",
    "\n",
    "    abs = {}\n",
    "    \n",
    "    for file in os.listdir(local_data_path):\n",
    "    \n",
    "        # skip anythingthat's not what we're looking for\n",
    "        if not file.endswith('txt'):\n",
    "            continue \n",
    "    \n",
    "        fp = f'{local_data_path}/{file}'\n",
    "    \n",
    "        data = open(fp).read()\n",
    "    \n",
    "        # split based on seeing the word \"Introduction\"\n",
    "        if 'Introduction' in data:\n",
    "            abstract = data.split('Introduction')[0].replace ('\\n', ' ')\n",
    "            abs[fp] = {'abstract':abstract, 'length':len(abstract)}        \n",
    "    \n",
    "        elif 'INTRODUCTION' in data:\n",
    "            abstract = data.split('INTRODUCTION')[0].replace ('\\n', ' ')\n",
    "            abs[fp] = {'abstract':abstract, 'length':len(abstract)}      \n",
    "    return abs\n",
    "\n",
    "abstracts =  papers_etl('data')\n",
    "check_abstracts(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b91d0c2-e4d3-4db9-80cb-db1ca532715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LEVERAGING CONFIDENCE MODELSFOR IDENTIFYING CHALLENGING DATA SUBGROUPS IN SPEECH MODELSAlkis Koudounas♣, Eliana Pastor♣, Vittorio Mazzia♡, Manuel Giollo♡,Thomas Gueudre♡, Elisa Reale♡, Giuseppe Attanasio♢, Luca Cagliero♣,Sandro Cumani♣, Luca de Alfaro♠, Elena Baralis♣, Daniele Amberti♡♣Politecnico d'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts['data/nine.txt']['abstract'][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f4b77-0a06-4150-a9f4-be17b77bdccd",
   "metadata": {},
   "source": [
    "### Step 3. Format into a manifest file\n",
    "We'll be sending these files into the mechanical turk workforce using SageMaker Ground Truth, which has a managed interface for setting up the job incluidng a nice page for question answering. This needs a manifest file, which is simply a jsonlines object with all of the abstracts. Let's create that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebe1b1e7-ba6b-4dd7-8cf1-66604033c824",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2972b4c-b5f7-4f3d-9498-0b6b0758f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "with jsonlines.open('abstracts.manifest', 'w') as writer:\n",
    "\n",
    "    for data in abstracts.values():\n",
    "\n",
    "        # SM GT wants to see the word source as the key\n",
    "        writer.write({'source': data['abstract']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b8ea0f3-58ad-4cbd-9625-eedba00f3cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./abstracts.manifest to s3://dist-train/amazon-science/Amazon Science Training Data/labelling-job/abstracts.manifest\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp abstracts.manifest '{s3_path}/labelling-job/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485cce2-0990-4761-98bd-44b9028388d8",
   "metadata": {},
   "source": [
    "### Step 4. Start a labelling job!\n",
    "Next, we'll navigate to the SageMaker Ground Truth labelling job page to get this running."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
